{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Project_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricglz/CE888_activities/blob/main/assignment/Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR0i3ECTUmmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258984ed-a846-4244-84ef-a38ca36ceeb5"
      },
      "source": [
        "! pip install torch torchvision skorch timm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n",
            "Requirement already satisfied: skorch in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.6/dist-packages (0.3.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.4.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu0y8g8gUmmz"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXhVa-RwUmmz"
      },
      "source": [
        "Before we begin, lets mount the google drive to later on read information from it:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeqA16HnUmmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7450ad1c-a931-4697-8008-3ea570378737"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive_path = '/content/gdrive'\n",
        "drive.mount(drive_path, force_remount=False)\n",
        "drive_path += '/MyDrive'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuutjrXRcWd7"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iixFi55mUmm1"
      },
      "source": [
        "## The Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sBrFemBUmm1"
      },
      "source": [
        "We are going to train a neural network to classify **ants** and **bees**. The dataset consist of 120 training images and 75 validiation images for each class. First we create the training and validiation datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDmTwvy1Umm2"
      },
      "source": [
        "import torchvision.transforms as T\n",
        "from os import path\n",
        "\n",
        "data_dir = path.join(drive_path, 'Flame')\n",
        "resize = T.Resize((254, 254))\n",
        "normalize = T.Normalize([0.485, 0.456, 0.406], \n",
        "                         [0.229, 0.224, 0.225])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HStp5BCmbkI"
      },
      "source": [
        "train_transforms = T.Compose([\n",
        "  resize,\n",
        "  T.RandomHorizontalFlip(),\n",
        "  T.RandomVerticalFlip(),\n",
        "  T.ToTensor(),\n",
        "  normalize\n",
        "])\n",
        "transforms = T.Compose([\n",
        "  resize,\n",
        "  T.ToTensor(),\n",
        "  normalize\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxTIbTj_mXxM",
        "outputId": "f38a6e9d-c3b2-4bcd-ab3b-8925c03d3565"
      },
      "source": [
        "import torchvision.datasets as datasets\n",
        "train_ds = datasets.ImageFolder(path.join(data_dir, 'Training'),\n",
        "                                train_transforms)\n",
        "len(train_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80922"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAGsjjWgx24",
        "outputId": "eb5d7494-a036-4af7-cf53-260b9b0473d0"
      },
      "source": [
        "test_ds = datasets.ImageFolder(path.join(data_dir, 'Test'), transforms)\n",
        "len(test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8617"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N99nSb3GUmm2"
      },
      "source": [
        "The train dataset includes data augmentation techniques such as cropping to size 224 and horizontal flips.The train and validiation datasets are normalized with mean: `[0.485, 0.456, 0.406]`, and standard deviation: `[0.229, 0.224, 0.225]`. These values are the means and standard deviations of the ImageNet images. We used these values because the pretrained model was trained on ImageNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F961Gb0ZUmm3"
      },
      "source": [
        "## Loading pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mVpUMblUmm3"
      },
      "source": [
        "We use a pretrained `ResNet18` neural network model with its final layer replaced with a fully connected layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd7xePlvUmm4"
      },
      "source": [
        "from torch import load, FloatTensor\n",
        "from torch.nn import Linear, Module\n",
        "import timm\n",
        "\n",
        "f_params = None\n",
        "\n",
        "class PretrainedModel(Module):\n",
        "    def __init__(self, model='rexnet'):\n",
        "        super().__init__()\n",
        "        model_name = self.get_model_name(model)\n",
        "        self.model = timm.create_model(model_name, pretrained=True, num_classes=1)\n",
        "        # if use_pretrained:\n",
        "        #    self.model.load_state_dict(self.get_state_dict())\n",
        "    \n",
        "    def get_state_dict(self):\n",
        "        remove_model_prefix = lambda string: string[6:]\n",
        "        return { remove_model_prefix(k): v for k, v in load(f_params).items() }\n",
        "      \n",
        "    def get_model_name(self, general_model):\n",
        "        return 'rexnet_200' if general_model == 'rexnet' else \\\n",
        "               'tf_efficientnet_b8' if general_model == 'efficientnet' else ''\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).squeeze(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t7R8UgSUmm4"
      },
      "source": [
        "Since we are training a binary classifier, the output of the final fully connected layer has size 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WEDynHWUmm5"
      },
      "source": [
        "## Defining the API\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyQ3D-5uUmm5"
      },
      "source": [
        "### Callbacks\n",
        "\n",
        "In this case the only Callback that will be used in every model will be an early stopping callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U38wxdqvuuan"
      },
      "source": [
        "from skorch.callbacks import EarlyStopping, Freezer, LRScheduler, ProgressBar\n",
        "\n",
        "is_top_layer = lambda x: not x.startswith('model.fc') and \\\n",
        "                            not x.startswith('model._fc') and \\\n",
        "                            not x.startswith('model.head') and \\\n",
        "                            not x.startswith('model.classifier')\n",
        "freezer = Freezer(is_top_layer) \n",
        "early_stopping = EarlyStopping(patience=3)\n",
        "scheduler = LRScheduler(policy='StepLR', gamma=9e-1, step_size=1)\n",
        "progress_bar = ProgressBar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9b7Q8CkUmm8"
      },
      "source": [
        "### Helper functions classifier\n",
        "\n",
        "The next code will be used to create helper functions to easily create, fit and evaluate different type of CNN architectures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouQuUaZduCI8"
      },
      "source": [
        "from torch import float64\n",
        "from skorch.classifier import NeuralNetBinaryClassifier\n",
        "from skorch.utils import to_tensor, to_numpy\n",
        "import sklearn.metrics as sk_metrics \n",
        "import numpy as np\n",
        "\n",
        "class MyClassifier(NeuralNetBinaryClassifier):\n",
        "    def infer(self, x, **fit_params):\n",
        "        x = to_tensor(x, device=self.device)\n",
        "        if isinstance(x, dict):\n",
        "            x_dict = self._merge_x_and_fit_params(x, fit_params)\n",
        "            return self.module_(**x_dict).to(device=self.device, dtype=float64)\n",
        "        return self.module_(x, **fit_params).to(device=self.device, dtype=float64)\n",
        "\n",
        "    def train_step_single(self, Xi, yi, **fit_params):\n",
        "        self.module_.train()\n",
        "        y_pred = self.infer(Xi, **fit_params)\n",
        "        yi = yi.to(device=self.device, dtype=float64)\n",
        "        loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n",
        "        loss.backward()\n",
        "        return { 'loss': loss, 'y_pred': y_pred }\n",
        "\n",
        "    def validation_step(self, Xi, yi, **fit_params):\n",
        "        self.module_.eval()\n",
        "        y_pred = self.infer(Xi, **fit_params)\n",
        "        yi = yi.to(device=self.device, dtype=float64)\n",
        "        loss = self.get_loss(y_pred, yi, X=Xi, training=False)\n",
        "        return { 'loss': loss,'y_pred': y_pred }\n",
        "\n",
        "    def _get_y_values(self, X):\n",
        "        y_true, y_pred = [], []\n",
        "        nonlinearity = self._get_predict_nonlinearity()\n",
        "        for images, labels in self.get_iterator(X):\n",
        "            images = images.to(self.device)\n",
        "            outputs = nonlinearity(self.module_(images))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            y_true.append(to_numpy(labels))\n",
        "            y_pred.append(to_numpy(predicted))\n",
        "        y_true = np.concatenate(y_true)\n",
        "        y_pred = np.concatenate(y_pred)\n",
        "        return y_true, y_pred\n",
        "\n",
        "    def score(self, X, y=None):\n",
        "        y_true, y_pred = self._get_y_values(X)\n",
        "        return sk_metrics.roc_auc_score(y_true, y_pred)\n",
        "    \n",
        "    def scores(self, X, y=None):\n",
        "        y_true, y_pred = self._get_y_values(X)\n",
        "        accuracy = sk_metrics.accuracy_score(y_true, y_pred)\n",
        "        confusion_matrix = sk_metrics.confusion_matrix(y_true, y_pred)\n",
        "        f1 = sk_metrics.f1_score(y_true, y_pred)\n",
        "        auc = sk_metrics.roc_auc_score(y_true, y_pred)\n",
        "        return accuracy, confusion_matrix, f1, auc "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZj_giOyUmm8"
      },
      "source": [
        "from torch.optim import Adam\n",
        "from skorch.callbacks import Checkpoint\n",
        "from skorch.dataset import CVSplit\n",
        "\n",
        "def create_model(module_model):\n",
        "    global f_params\n",
        "\n",
        "    f_params = path.join(drive_path, f'Models/best_{module_model}.pt')\n",
        "    checkpoint = Checkpoint(f_params=f_params, monitor='valid_acc_best')\n",
        "    callbacks = [checkpoint, freezer, early_stopping, scheduler]\n",
        "    lr = 2e-3\n",
        "\n",
        "    return MyClassifier(\n",
        "        PretrainedModel,\n",
        "        module__model=module_model,\n",
        "        optimizer=Adam,\n",
        "        lr=lr,\n",
        "        batch_size=28,\n",
        "        max_epochs=10,\n",
        "        iterator_train__shuffle=True,\n",
        "        iterator_train__num_workers=16,\n",
        "        iterator_valid__shuffle=True,\n",
        "        iterator_valid__num_workers=16,\n",
        "        train_split=CVSplit(0.2m, random_state=seed),\n",
        "        callbacks=callbacks,\n",
        "        device='cuda'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1nEzDdmi0br"
      },
      "source": [
        "def create_and_fit(model_name):\n",
        "    net = create_model(model_name)\n",
        "    net.fit(train_ds, y=None)\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKrfrOynM3gO"
      },
      "source": [
        "def print_and_plot_scores(net):\n",
        "    accuracy, confusion_matrix, f1, auc = net.scores(test_ds, y=None)\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'F1 Score: {f1}')\n",
        "    print(f'AUC: {auc}')\n",
        "    disp = sk_metrics.ConfusionMatrixDisplay(\n",
        "      confusion_matrix, display_labels=['Fire', 'No_Fire'])\n",
        "    disp.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II7x55KXUmm9"
      },
      "source": [
        "That is quite a few parameters! Lets walk through each one:\n",
        "\n",
        "1. `model_ft`: Our `ResNet18` neural network\n",
        "2. `criterion=nn.CrossEntropyLoss`: loss function\n",
        "3. `lr`: Initial learning rate\n",
        "4. `batch_size`: Size of a batch\n",
        "5. `max_epochs`: Number of epochs to train\n",
        "6. `module__output_features`: Used by `__init__` in our `PretrainedModel` class to set the number of classes.\n",
        "7. `optimizer`: Our optimizer\n",
        "8. `optimizer__momentum`: The initial momentum\n",
        "9. `iterator_{train,valid}__{shuffle,num_workers}`: Parameters that are passed to the dataloader.\n",
        "10. `train_split`: A wrapper around `val_ds` to use our validation dataset.\n",
        "11. `callbacks`: Our callbacks \n",
        "12. `device`: Set to `cuda` to train on gpu.\n",
        "\n",
        "Now we are ready to train our neural network:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVSReHWzghMG"
      },
      "source": [
        "## Resnext model\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz11nA6CgZfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1fa8097-6907-445c-813c-1a5eba10c489"
      },
      "source": [
        "rexnet = create_and_fit('rexnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  epoch    train_loss    valid_acc    valid_loss    cp      lr        dur\n",
            "-------  ------------  -----------  ------------  ----  ------  ---------\n",
            "      1        \u001b[36m0.3469\u001b[0m       \u001b[32m0.9665\u001b[0m        \u001b[35m0.1197\u001b[0m     +  0.0020  2425.1572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "      2        \u001b[36m0.1279\u001b[0m       \u001b[32m0.9781\u001b[0m        \u001b[35m0.0741\u001b[0m     +  0.0018  700.8887\n",
            "      3        \u001b[36m0.0905\u001b[0m       \u001b[32m0.9841\u001b[0m        \u001b[35m0.0559\u001b[0m     +  0.0016  699.2225\n",
            "      4        \u001b[36m0.0727\u001b[0m       \u001b[32m0.9854\u001b[0m        \u001b[35m0.0468\u001b[0m     +  0.0015  697.6781\n",
            "      5        \u001b[36m0.0618\u001b[0m       \u001b[32m0.9873\u001b[0m        \u001b[35m0.0408\u001b[0m     +  0.0013  699.2267\n",
            "      6        \u001b[36m0.0584\u001b[0m       \u001b[32m0.9896\u001b[0m        \u001b[35m0.0370\u001b[0m     +  0.0012  695.9418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWXJJtTkGqtO"
      },
      "source": [
        "print_and_plot_scores(rexnet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McSBPhJshE_U"
      },
      "source": [
        "## EfficientNet\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFGkLLS_hpYy"
      },
      "source": [
        "efficientnet = create_and_fit('efficientnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQJdDUwGGuq3"
      },
      "source": [
        "print_and_plot_scores(efficientnet)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}