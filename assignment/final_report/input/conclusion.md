# Conclusion

<!-- Summarize your key findings. Include important conclusions that can be drawn and further implications for the field. Discuss the benefits or shortcomings of your work and suggest future areas for research. -->

This paper developed four models that showed the necessity that exists of using efficient techniques and architectures to not only obtain a good model performance, while using less resources during the training. This allows researchers or other developers to implement these models in IoT devices, as the response will be faster than with models that demand more resources to operate. It could even allow the developers to add the software directly to the devices themselves, because based on the information in table \ref{tab:efficiency} we can assume that the inference time of most of the models, with the exception of EfficientNet, is faster than needed.

Now that we know that these techniques help in the creation of better models, a next step could be to replicate the procedure, we should replicate the procedure for architectures that were designed for mobile devices or those devices with limited resources. Architectures such as: MobileNet \cite{howard2019}, FBNet \cite{wu2019} and pruned versions of the less complex versions of EfficientNet \cite{tan2020}.

Nevertheless, there are still factors that could improve the performance of the models, starting with having a more diverse dataset. As we consider that one problem with the datasets is the repetition of the same frames, caused due to being extracted from videos. This procedure may cause that the model can't learn to generalize quickly enough to classify one label with the other, or maybe every if the dataset is of very poor quality. In addition that basically this create the illusion that our dataset may be big enough, able to generalize the information, while the reality may be that there are not enough samples of all the environments. As a proposal to solve this issue it would be to create a generative model, which creates images of environments with and without fire. With this technique there would be a dataset with a broad spectrum of environments that could lead better performance and generalization for the classification models.

As mentioned before, Adam was the most used optimizer in our models, despite SGD can be seen in some cases as the favorite for training. This opens a possible new research to answer the following question: In which circumstances SGD can be a better optimizer than Adam or RMSProp? As there are works such as \cite{keskar2017} in which it shows how SGD performs better than Adam. Could it be related to the dataset used or to the amount of epochs in which the run is performed?
