%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

\documentclass[conference]{IEEEtran}

% *** CITATION PACKAGES ***
\usepackage{cite}

% *** GRAPHICS RELATED PACKAGES ***
\usepackage{graphicx,subcaption}
\graphicspath{{./img}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

% *** ALIGNMENT PACKAGES ***
\usepackage{float, array}

% *** PDF, URL AND HYPERLINK PACKAGES ***
\usepackage[unicode=true,breaklinks,hidelinks]{hyperref}
\urlstyle{same}
\usepackage{csquotes}

% For tables
\usepackage{booktabs}

% For listing
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Verbatim
\usepackage{listings}
\lstset{
  frame=single,
  language=Python,
  basicstyle=\small,
}

\makeatletter
\def\lst@makecaption{%
  \def\@captype{table}%
  \@makecaption
}
\makeatother

\makeatletter
\def\lst@makecaption{%
  \def\@captype{table}%
  \@makecaption
}
\makeatother

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Improving Fire Detection with Efficient Deep Learning Training
Techniques}

% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs

\author{Ricardo~Gonzalez, 2003297}

% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
% {Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}

% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
Deep Learning and Convolutional Neural Networks are currently a popular
topic of research, leading to works that attempt to build models that
can be used in the world, instead of just being used in research.
Nevertheless, there are works that focus on this but don't achieve a
good performance, due to the use of inefficient architectures or not
applying techniques to improve the training and reduce the resources
used. In this work we will increase the accuracy of our baseline model,
with the use of newer architectures and by the use of new techniques
such as Mixed-Precision training, Mixup and Transfer Learning. The
result of this work were four new models that not only trained in fewer
amount of epochs, but also achieve higher accuracy than our baseline.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
    CNN;
    Deep Learning;
    Image Classification;
    Efficient Training\end{IEEEkeywords}}

% make the title area
\maketitle

% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc
% or transmag modes are not selected <OR> if conference mode is selected
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Nowadays due to the advancements in Artificial Intelligence (AI) and the
increasing amount of ecological disasters many researchers have focused
their attention in not only making new advancements in the theory of the
field, but also by proposing ways to solve, prevent or detect those
disasters to mitigate the impact that they have \cite{weber2020}.

A natural disaster is researched this way are wildfires. According to
\cite{NOAA2020}, wildfires produce emissions which are highly
contaminant, leading to an increase in air pollution not only in the
area affected by the wildfires, but also close areas near to it.
Furthermore, the natural fauna and flora are also threatened, by fires
caused from human intervention. This is explained by \cite{malhi2009},
who uses as an example the Amazon Rainforest, where the human caused
wildfires alongside droughts and have threatened the Rainforest to a
possible tipping point, where it could become unsustainable unless there
is some kind of intervention.

To tackle this problem, researchers have decided to work in the
detection of wildfires in their early stages such as in
\cite{shamsoshoara2020} and \cite{saied2020}, whose work have been
either proposing CNN models using their own datasets or creating a new
dataset containing images to create models that can be used to solve the
problem.

Nevertheless, the models normally are not very accurate, an example of
this is the one proposed by \cite{shamsoshoara2020} whose model achieved
76\%, which shows that it is still possible and worth to achieve a
higher score. Specially considering that the architecture used could be
seen as old, as new and better ones have been published since then. In
addition to the prior, the training process was a very simple training
process that was primarily based on training through a lot of epochs.
Thus, new techniques can be used to not only increase the accuracy but
also reduce the amount of epochs needed to train it.

That is why the objective of this research is to present, using new
architectures and better training techniques, models that can achieve
higher accuracy over the same test dataset as the one used in
\cite{shamsoshoara2020}, while needing less amount of epochs to be
trained.

\hypertarget{background-literature-review}{%
\section{Background / Literature
Review}\label{background-literature-review}}

\hypertarget{mixed-precision-training}{%
\subsection{Mixed-precision training}\label{mixed-precision-training}}

One of the current problems that people are facing nowadays with Deep
Learning (DL) is the amount of resources that it takes to train a model,
either because the architecture has a lot of parameters and takes a lot
of memory of the GPU. Or because it takes a lot of time to be trained
due to the computational power it needs. To solve this problem,
\cite{micikevicius2018} proposed what is known as mixed-precision
training, where instead of using the full-precision number of 8 bytes,
it would use the 4 byte format. As showed in \cite{he2019} using this
technique led to a reduction of the amount of memory it took to train
the model, in addition to a speedup in the time that the model took to
be trained.

\hypertarget{data-augmentation}{%
\subsection{Data Augmentation}\label{data-augmentation}}

Data Augmentation is a technique that improves the generalization of the
network. This is done by performing manipulation of the data, in the
case of images, this mean applying techniques such as: shearing,
rotation, saturation and others \cite{van2001}. This technique is
effective because it generates new data each time a new epoch is run;
thus, resulting quite effective when learning from an overall small
dataset. In addition to the prior, it also has been proven to be
effective as it adds randomness to the training and it reduces the
changes that the same batch have the same data each epoch as was shown
in \cite{perez2017}.

\hypertarget{autoaugment}{%
\subsubsection{AutoAugment}\label{autoaugment}}

Nevertheless, one limitation this technique has is that its
effectiveness depends on the augmentations done over the data, as well
as choosing the correct parameters, mainly their magnitude and
probability. A solution of this limitation is proposed by
\cite{cubuk2019} who created a procedure called \emph{AutoAugment} that
created a search space consisting of a policy which consisted of
sub-policies that decide which augmentation to do and which are their
parameters. This resulted in an improvement of the previous
state-of-the-art models.

\hypertarget{test-time-augmentation}{%
\subsubsection{Test Time Augmentation}\label{test-time-augmentation}}

Even though, data augmentation is commonly used only for the training
phase it also has a purpose during the testing phase. This technique is
called Test Time Augmentation (TTA), in which the input is augmented and
passed as an input for the model n times to result in a total of n
outputs. With these outputs, a merge operation is then performed, which
normally is a mean function. This merge result is then used to obtain
the desired test metrics \cite{kim2020}.

\hypertarget{mixup}{%
\subsection{Mixup}\label{mixup}}

\begin{figure}[h]
\centering
\subcaptionbox{Before Mixup}{\includegraphics[width=0.2\textwidth]{img/before_mixup}}%
\hfill
\subcaptionbox{After Mixup}{\includegraphics[width=0.2\textwidth]{img/after_mixup}}%
\caption{Example of transformation of an image when using mixup}
\label{fig:mixup}
\end{figure}

Mixup is a technique that was proposed by \cite{zhang2018}, that aims to
help in the stabilization of adversarial networks in generative models;
nevertheless, it has also found success in classification tasks as was
demostrated in \cite{gong2020}. The technique consists of mixing both
the data and labels of elements in the batch, resulting in an overall
generalization of how the distribution of the data would look for two
different elements of the same or different class. An example of the
result of mixup can be seen in figure \ref{fig:mixup}.

\hypertarget{transfer-learning}{%
\subsection{Transfer learning}\label{transfer-learning}}

As said by \cite{torrey2010} ``Transfer learning is the improvement of
learning in a new task through the transfer of knowledge from a related
task that has already been learned''. This technique helps in reducing
the amount of computational time to achieve the same or better accuracy.
By using the weights of a model with the same architecture trained for
another task, the model will use that prior knowledge to learn this new
task faster, proven by the works of \cite{shao2018}, \cite{zoph2016} and
\cite{chiang2020}.

\hypertarget{onecyclelr}{%
\subsection{OneCycleLR}\label{onecyclelr}}

\begin{figure}
\centering
\subcaptionbox{Learning rate behavior}{\includegraphics[width=0.24\textwidth]{img/one_cycle_lr}}%
\hfill
\subcaptionbox{Momentum behavior}{\includegraphics[width=0.24\textwidth]{img/one_cycle_momentum}}%
\caption{Behavior of momentum and LR when using OneCycleLR. Plots done by \cite{gugger2018}}
\label{fig:one_cycle}
\end{figure}

Proposed by \cite{smith2018}, OneCycleLR is a scheduler that was created
to achieve what \cite{smith2018} called as ``Super-Convergence'', where
the model achieves an improvement of the accuracy in less epochs.

The scheduler, as can be seen at figure \ref{fig:one_cycle}, doesn't
only consist of modifying the learning rate as most schedulers do, but
it also modifies the momentum. In the case of the learning rate, it
starts with an initial value, which will increase by each step until it
reaches a max value; conventionally this is at the middle of the
training, but can be at any other point. Then, the learning rate will
decrease until reaching a value lower than the initial value. In the
case of the momentum, the behavior is the opposite of the learning rate,
starting with a very high value, then descending and finally going back
again to the max value.

The scheduler has proven its usefulness in reducing the amount of
iterations need, while keeping or increasing the accuracy in the works
of \cite{kuo2020} and \cite{kim2020Pynet}. In the case of the former it
was possible to reduce the amount of epochs needed by a 68\% compared to
their baseline, while also improving the model's accuracy.

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

\hypertarget{dataset}{%
\subsection{Dataset}\label{dataset}}

The dataset used was a merge between the datasets done by
\cite{Flame2020}, \cite{saied2020} and \cite{dunnings18}. The code to
perform the same preprocessing is fully available on Github
\footnote{\url{https://github.com/ricglz/CE888_activities/blob/main/assignment/scripts/data_preprocessing.py}}.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|r|}
\toprule
Dataset & Fire & No Fire & Total \\
\midrule
Train/Val & 25018 (63.54\%) & 14357 (36.46\%) & 39375 (100.00\%) \\
Test & 5137 (59.61\%) & 3480 (40.39\%) & 8617 (100.00\%) \\
\bottomrule
\end{tabular}
\caption{FLAME dataset distribution}
\label{tab:1}
\end{table}

\hypertarget{flame-dataset}{%
\subsubsection{FLAME dataset}\label{flame-dataset}}

The FLAME dataset consists of 47,992 images that are labeled as having
fire or not. 39,375 of the total amount of images are for
training/validation. As can be seen at table \ref{tab:1}, the
training/validation set, the labels are skewed towards the class with
fire. These images were obtained by the researchers by extracting the
frames of videos recorded by drones of forest areas \cite{Flame2020}.

\hypertarget{fires-dataset}{%
\subsubsection{FIRE's dataset}\label{fires-dataset}}

This dataset was created for a NASA challenge in 2018, the authors
collected a total of 1,000 images, all labeled for training data. These
images, contrary to the previous dataset are from a wide range of
environments, from urban to rural areas. Nevertheless, the dataset is
skewed, containing 755 images labeled as fire and the rest as no-fire
\cite{saied2020}.

\hypertarget{dunnings-dataset}{%
\subsubsection{Dunning's dataset}\label{dunnings-dataset}}

The dataset was created by Dunning et al.~consisting of 23,408 images
for training. This dataset was created by merging other datasets and
material from public videos \cite{dunnings18}. This dataset also has a
skew over the fire images.

\hypertarget{merging-datasets}{%
\subsubsection{Merging datasets}\label{merging-datasets}}

All the images of the FIRE's and Dunning's dataset were merged into the
training/validation dataset of flame.

\hypertarget{balancing-the-datasets}{%
\subsubsection{Balancing the datasets}\label{balancing-the-datasets}}

After merging the datasets, the next part of the preprocessing was to
balance the dataset; because, as mentioned in the prior sections, all
the datasets are skewed towards the label with fire. To balance the
dataset, we over-sample the no fire class label by performing Data
Augmentation over random samples of the label. The augmentations done to
the dataset were brightness, contrast, rotation, horizontal and vertical
flip. This resulted in a dataset containing 76,726 images with a perfect
balance between the 2 classes.

\hypertarget{dividing-trainingvalidation}{%
\subsubsection{Dividing
Training/Validation}\label{dividing-trainingvalidation}}

The next step would be to split the training/validation dataset into
their own predefined folders, this will help by always using the same
images for training and validation, instead of random ones. Therefore
the dataset
\footnote{Dataset before halving training: \url{https://drive.google.com/file/d/1uv9vAl55IinuEMXHocnJQUhPbMikuSIX}}
was split into 80\% training and 20\% validation, keeping the balanced
ratios between the labels.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|r|}
\toprule
Dataset & Fire & No Fire & Total \\
\midrule
Train & 15341 (50\%) & 15341 (50\%) & 30682 (100.00\%) \\
Validation & 7671 (50\%) & 7671 (50\%) & 15342 (100.00\%) \\
Test & 5137 (59.61\%) & 3480 (40.39\%) & 8617 (100.00\%) \\
\bottomrule
\end{tabular}
\caption{Dataset distribution after preprocessing}
\label{tab:2}
\end{table}

\hypertarget{reducing-the-amount-of-data-in-training}{%
\subsubsection{Reducing the amount of data in
training}\label{reducing-the-amount-of-data-in-training}}

With a total of 61,378 images, there was a lot of data to process. If we
want that the training to be as efficient as possible we need to reduce
the amount of data used for training. As there are a lot of images that
are very similar between each other, due to being frames extracted from
videos. Then it was decided to cut the amount of training data into
half, while keeping the ratio of classes as before. This was the last
step for the creation of the dataset
\footnote{Dataset after halving training: \url{https://drive.google.com/file/d/1RrO4boe9jHUsCY1l9Z55iG1sfydJzubs/view}}
and resulted in a distribution as it shows in table \ref{tab:2}

\hypertarget{model}{%
\subsection{Model}\label{model}}

For this paper we will experiment with different architectures as the
backbone of our model. Taking in consideration that
\cite{shamsoshoara2020} used the Xception \cite{chollet2017}
architecture as the backbone of their model, which could be considered
an old architecture. Therefore it is important to look for newer models
which are more efficient in terms of time of inference, training and
amount of parameters.

These conditions reduced the experimentation to the next architectures:
EfficientNet \cite{tan2020}, ReXNet \cite{han2020}, GENet \cite{lin2020}
and RepVGG \cite{ding2021}.

With these architectures we will perform transfer learning to be able to
learn from the current task. It must be mentioned that all of these
architectures have already been trained and have been shared by
\cite{timm2019} who have also shared the code to use the models.

\begin{table*}[t]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
\toprule
& \multicolumn{2}{|c|}{Training} & \multicolumn{2}{|c|}{Validation} & \multicolumn{2}{|c|}{Test}\\
\cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7}
Model & Accuracy (\%) & Loss & Accuracy (\%) & Loss & Accuracy (\%) & Loss \\
\midrule
FLAME & 96.79 & 0.0857 & 94.31 & 0.1506 & 76.23 & 0.7414 \\
GENet & 73.05 & 0.3925 & 99.15 & 0.0749 & 83.17 & 0.3722 \\
EfficientNet & 97.94 & 0.006275 & 99.62 & 0.0293 & 83.18 & 0.4198 \\
RepVGG & 98.58 & 0.2313 & 98.74 & 0.1525 & 0.8463 & 0.5854 \\
ReXNet & 99.23 & 0.0855 & 99.07 & 0.02632 & 90.68 & 0.2259 \\
\bottomrule
\end{tabular}
\caption{Models accuracies and losses in all datasets}
\label{tab:performance}
\end{table*}

\hypertarget{training}{%
\subsection{Training}\label{training}}

The model will be trained for 5 epochs using Mixed Precision and
OneCycleLR as a learning scheduler. In addition to augmenting the data,
the images will be resized to a ratio of 224x224 and normalize the
values based on the mean and standard deviation of the Imagenet dataset
\footnote{$mean = [0.485, 0.456, 0.406]$ and $std=[0.229, 0.224, 0.225]$}.
As this was the one used in which the models were pre-trained.

Also we used a special data loader which will create batches containing
the same amount of random elements of each class. This was possible due
to the previous work of \cite{galato2019}, who developed a similar
sampler for their use case. Also depending of the model, it will be
trained using mixup.

The optimizer for the training will be either SGD, Adam or RMSProp,
depending of how the tuning of the hyper-parameters turns out.
Meanwhile, the loss function will depend if the model is trained by
using mixup or not. When using Mixup it will be trained by using
CrossEntropyLoss, meanwhile if it's not trained by it, the model will be
trained using BCEWithLogitsLoss.

When the training has ended we will restore the weights of the model to
the epoch in which it had the best score, which we consider to be the
average between the accuracy and F1 score in the test dataset.

In addition the complete code can be found on Github
\footnote{\url{https://github.com/ricglz/CE888_activities/tree/main/assignment/scripts}},
where it can be found the scripts used to perform the experiments.

\hypertarget{tta}{%
\subsubsection{TTA}\label{tta}}

For the test dataset we will use TTA. The merge function that will be
used is the mean function. Meanwhile, the amount of augmentations to
perform will depend on the backbone of the model. The augmentations to
perform will be determined by AutoAugment policies.

\hypertarget{data-augmentation-1}{%
\subsubsection{Data Augmentation}\label{data-augmentation-1}}

For training the model either custom AutoAugment policies will be used
or random vertical and horizontal flips, rotations of 45º and
modifications in the brightness and contrast of the photos.

\hypertarget{fine-tuning}{%
\subsubsection{Fine-tuning}\label{fine-tuning}}

As part of the training the model is not completely trainable from the
start. At the start, only the first N layers will be trainable while the
others will be frozen. At the end of each epoch the next N layers will
be unfrozen, until either all the layers are now trainable or the
training has been completed. N will depend of the model, as well as if
the BatchNormalizer layers will also be trainable or not.

\hypertarget{hyper-parameters-tuning}{%
\subsection{Hyper-parameters tuning}\label{hyper-parameters-tuning}}

As we have architectures as backbone that have an inference time small
enough to be able to iteratively tune the hyper-parameters of the model,
it was decided to do it. This was done by using Weight and Biases
(wandb) \cite{wandb2020}, which is a tool that allows to log models,
their metrics and, more importantly for our case, to create
\emph{``sweeps''}. These \emph{``sweeps''} work by defining a search
space that allows training models with different hyper-parameters with
one or many machines. In these ``sweeps'' it allows the user to use
either grid, random or bayes search to generate the hyper-parameters to
experiment with. In our case most of the time random search was enough,
but there were occasions where a bayes or grid search were used instead.
The hyper-parameters that were tuned were the following:

\begin{itemize}
\tightlist
\item
  Amount of augmentations to do for TTA
\item
  Drop rate
\item
  Optimizer
\item
  Optimizer's weight decay
\item
  Mixup's alpha
\item
  If BatchNormalizer layers will be trained or not
\item
  AutoAugment hyper-parameters:

  \begin{itemize}
  \tightlist
  \item
    If AutoAugment or pre-defined augmentations will used
  \item
    Amount of augmentations per policy
  \item
    Magnitude of augmentations per policy
  \item
    Probability of augmentations
  \end{itemize}
\item
  OneCycleLR hyper-parameters:

  \begin{itemize}
  \tightlist
  \item
    Anneal strategy to use (linear or cos)
  \item
    Min momentum value
  \item
    Max momentum value
  \item
    Max learning rate value
  \item
    Div factor to determine the initial learning rate
  \item
    Div factor to determine the minimum learning rate
  \item
    When the learning rate will start decreasing
  \end{itemize}
\item
  Fine-tuning hyper-parameters:

  \begin{itemize}
  \tightlist
  \item
    How many layers to unfreeze per epoch
  \end{itemize}
\end{itemize}

\hypertarget{results}{%
\section{Results}\label{results}}

The following results were obtained by using NVidia RTX 2080 gpu for the
ReXnet model and a GTX 1080 TI for the other.

\hypertarget{results-compared-to-baseline}{%
\subsection{Results compared to
baseline}\label{results-compared-to-baseline}}

\begin{figure}[h]
\centering
\subcaptionbox{Training loss through steps \label{fig:train-loss}}{\includegraphics[width=0.24\textwidth, height=1.5in]{img/training_loss}}%
\hfill
\subcaptionbox{Validation loss through steps \label{fig:val-loss}}{\includegraphics[width=0.24\textwidth, height=1.5in]{img/validation_loss}}%
\caption{Progression of the loss in training and validation datasets}
\label{fig:dataset-losses}
\end{figure}

As we can see in figure \ref{fig:train-loss}, the reduction of the
training loss seems normal among most of the models, the GENet model
being the exception. This is because the model was the only one that was
trained by using mixup. Thus, even though it has overall a high loss
through all the training, the loss is also very stable without a lot of
peaks, just like the ones that ReXNet and EfficientNet have at the start
of the training.

We will consider the model done in \cite{shamsoshoara2020} as our
baseline to determine if the tuning and new techniques, actually improve
the model performance over what is the current state-of-the-art. As it
can be seen in table \ref{tab:performance}, all the models perform
better than the FLAME model in the test dataset, as both the accuracy
and the loss in the dataset improve. Nevertheless, we have a special
case that is the GENet model, in which the performance in the training
dataset is considerably lower than the other models, even though it
achieved a higher accuracy than the FLAME model in the test dataset.
This is because GENet is the only model that used mixup as part of the
training, as a result of the tuning of the hyper-parameters.

\hypertarget{efficient-training-results}{%
\subsection{Efficient training
results}\label{efficient-training-results}}

\begin{table}[h]
\centering
\begin{tabular}{llp{2cm}}
\toprule
Model & Time Taken (minutes) & Maximum GPU Memory Allocated (\%) \\
\midrule
GENet & 9.8 & 16.92 \\
EfficientNet & 26.1 & 18.98 \\
RepVGG & 9.89 & 15.81 \\
ReXNet & 8.22 & 62.92 \\
\bottomrule
\end{tabular}
\caption{Time and memory metrics of each model}
\label{tab:efficiency}
\end{table}

In addition to the performance metrics, it will be analyzed the
computation time to train the model and the maximum percentage of memory
allocated. And as it can be seen in table \ref{tab:efficiency}, it is
very impressive that most of the models used can be trained in less than
10 minutes. A comparison with the FLAME model is not possible as
\cite{shamsoshoara2020} only shared the amount of epochs needed to
train, which were 40. But taking into consideration that in average we
took around 3 minutes per epoch to train, then we can assume that
probably their model was trained in around 120 minutes, but the real
number may be higher.

Regarding the memory consumption, there are 2 factors to consider prior
to analyzing the results. First the GPUs memories, the GTX 1080 Ti has
11GB of memory, meanwhile the RTX 2080 that was the one used for the
ReXNet training has only 8GB. Secondly, the ReXNet model was trained in
a multi-node GPU which later on was discovered had a problem of
allocating the same GPU to different process. Taking that in
consideration is very interesting how all of the models are using less
than 5GB, as even though the ReXNet model is reporting a use of 62.92\%
of the memory it can be assume that part of it is used for another
process as the percentage is significantly higher than the other models.
Also it means, that we could have experimented with higher batch sizes
and analyze if it would improve or decrease the performance of the
model.

\hypertarget{hyper-parameter-tuning-insights}{%
\subsection{Hyper-Parameter tuning
insights}\label{hyper-parameter-tuning-insights}}

After tuning the hyper-parameters an insight concluded is that even
though a lot of papers decide to use SGD as their optimizer, in our case
none of the models ended up using it. Instead, the most popular
optimizer for our models was Adam, which was the one used for most of
the models, with the exception of RepVGG which used RMSProp.

In addition to the prior, it was interesting to see that mixup was
almost non-effective, as it was only used in the GENet. The reason could
be because there were only two classes; thus, reducing the effectiveness
of the technique. Also it can be concluded based on the models, that if
the OneCycleLR is used, it's possible that the best anneal strategy to
use is a linear behavior instead of a cosine. Additionally, also for the
OneCycleLR, the minimum value of the momentum should be between 0.75 and
0.82, meanwhile the max value should be 0.86 and 0.95

Finally, for this dataset the models require a drop rate between 0.45
and 0.55, as all the models have the models their best drop rate in this
range.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

This paper developed four models that showed the necessity that exists
of using efficient techniques and architectures to not only obtain a
good model performance, while using less resources during the training.
This allows researchers or other developers to implement these models in
IoT devices, as the response will be faster than with models that demand
more resources to operate. It could even allow the developers to add the
software directly to the devices themselves, because based on the
information in table \ref{tab:efficiency} we can assume that the
inference time of most of the models, with the exception of
EfficientNet, is faster than needed.

Now that we know that these techniques help in the creation of better
models, a next step could be to replicate the procedure, we should
replicate the procedure for architectures that were designed for mobile
devices or those devices with limited resources. Architectures such as:
MobileNet \cite{howard2019}, FBNet \cite{wu2019} and pruned versions of
the less complex versions of EfficientNet \cite{tan2020}.

Nevertheless, there are still factors that could improve the performance
of the models, starting with having a more diverse dataset. As we
consider that one problem with the datasets is the repetition of the
same frames, caused due to being extracted from videos. This procedure
may cause that the model can't learn to generalize quickly enough to
classify one label with the other, or maybe every if the dataset is of
very poor quality. In addition that basically this create the illusion
that our dataset may be big enough, able to generalize the information,
while the reality may be that there are not enough samples of all the
environments. As a proposal to solve this issue it would be to create a
generative model, which creates images of environments with and without
fire. With this technique there would be a dataset with a broad spectrum
of environments that could lead better performance and generalization
for the classification models.

As mentioned before, Adam was the most used optimizer in our models,
despite SGD can be seen in some cases as the favorite for training. This
opens a possible new research to answer the following question: In which
circumstances SGD can be a better optimizer than Adam or RMSProp? As
there are works such as \cite{keskar2017} in which it shows how SGD
performs better than Adam. Could it be related to the dataset used or to
the amount of epochs in which the run is performed?

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliographystyle{IEEEtran}
\bibliography{bibliography}
\end{document}
