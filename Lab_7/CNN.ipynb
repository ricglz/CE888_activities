{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricglz/CE888_activities/blob/main/Lab_6/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwRGEQbzGpYQ"
      },
      "source": [
        "# First CNN model for MNIST Dataset\n",
        "\n",
        "* MNIST Dataset is ''Hello World'' of Image Recognition\n",
        "\n",
        "* [Dataset HomePage](http://yann.lecun.com/exdb/mnist/)\n",
        "\n",
        "* History of MNIST Dataset [Watch here](https://www.youtube.com/watch?v=oKzNUGz21JM)\n",
        "\n",
        "\n",
        "---\n",
        "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a \n",
        "test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
        "\n",
        "![Kitten](https://camo.githubusercontent.com/01c057a753e92a9bc70b8c45d62b295431851c09cffadf53106fc0aea7e2843f/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhmJOHCpJD_w"
      },
      "source": [
        "# Let's start building our first CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSyHCSV7jymI"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWNzCYUUjymN"
      },
      "source": [
        "Importantly, a convnet takes as input tensors of shape (image_height, image_width,\n",
        "image_channels) (not including the batch dimension). In this case, we’ll configure\n",
        "the convnet to process inputs of size (28, 28, 1), which is the format of MNIST\n",
        "images. We’ll do this by passing the argument input_shape=(28, 28, 1) to the first\n",
        "layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM4JLEpwjymN"
      },
      "source": [
        "#### Instantiating a small convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-OnpExGjymO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e1f6ef-b3e4-40cb-d2fc-60d0b8a83065"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gcVG3xkjymR"
      },
      "source": [
        "#### Adding a classifier on top of the convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2DfhDJYjymR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be81e10-2496-4762-9164-e9ab0646b022"
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOKVF4nKjymU"
      },
      "source": [
        "### Training the convnet on MNIST images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIcgUbbUjymV"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnJ2Pfs_jymX"
      },
      "source": [
        "#### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpHGHE9MjymY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fbaa34-2a1f-4d06-edb9-613c3a9f07b9"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HoTLrfSjymd"
      },
      "source": [
        "#### compile and fit model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i23FDtC9jyme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1036b7c8-8654-4874-f345-26bcbae4ca58"
      },
      "source": [
        "model.compile(optimizer='rmsprop', \n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "750/750 [==============================] - 34s 4ms/step - loss: 0.4586 - accuracy: 0.8560 - val_loss: 0.0570 - val_accuracy: 0.9834\n",
            "Epoch 2/5\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0534 - accuracy: 0.9827 - val_loss: 0.0486 - val_accuracy: 0.9861\n",
            "Epoch 3/5\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0358 - accuracy: 0.9891 - val_loss: 0.0523 - val_accuracy: 0.9844\n",
            "Epoch 4/5\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0280 - accuracy: 0.9914 - val_loss: 0.0558 - val_accuracy: 0.9842\n",
            "Epoch 5/5\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0203 - accuracy: 0.9937 - val_loss: 0.0375 - val_accuracy: 0.9896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zU8iI5ojymg"
      },
      "source": [
        "#### evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3VeaL1Njymh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e904b9f-2947-4109-ccc3-977271c9cd51"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "test_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0335 - accuracy: 0.9892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9891999959945679"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXNZOY7Sjymj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "56c5bf80-b41d-44ed-f541-eb38e17e2c53"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsQMc0Iojyml"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "Change the activation function and other parameters such as optimizer to see the effect on the network and it's performance. If possible create a grid search. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdLSXNsxz94a"
      },
      "source": [
        "def create_model(optimizer='rmsprop', dropout_rate=0.5, activation='relu'):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(64, activation=activation))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owJ-29BrJXNa"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "model = KerasClassifier(\n",
        "    build_fn=create_model, verbose=1, validation_split=0.2, nb_epoch=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRdRaOXx0ngq"
      },
      "source": [
        "optimizer = ['rmsprop', 'adam', 'sgd']\n",
        "activation = ['relu', 'linear', 'tanh']\n",
        "dropout_rate = [0, 0.25, 0.5]\n",
        "batches = [32, 64, 128]\n",
        "param_grid = dict(\n",
        "    optimizer=optimizer, batch_size=batches, activation=activation,\n",
        "    dropout_rate=dropout_rate\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCSTc8hT4nOT"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "import numpy as np\n",
        "\n",
        "def scoring(y_true, y_pred):\n",
        "    y_true = np.argmax(y_true, axis=1)\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "my_scorer = make_scorer(scoring)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH_ExT7518st",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9642bfb6-ea26-4a9e-bdab-77050f908e1b"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=my_scorer)\n",
        "clf.fit(train_images, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1093 - accuracy: 0.7975 - val_loss: 0.0127 - val_accuracy: 0.9820\n",
            " 71/375 [====>.........................] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0956 - accuracy: 0.8302 - val_loss: 0.0148 - val_accuracy: 0.9799\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1031 - accuracy: 0.8089 - val_loss: 0.0127 - val_accuracy: 0.9823\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0978 - accuracy: 0.8252 - val_loss: 0.0127 - val_accuracy: 0.9828\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0976 - accuracy: 0.8301 - val_loss: 0.0167 - val_accuracy: 0.9745\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1121 - accuracy: 0.7903 - val_loss: 0.0130 - val_accuracy: 0.9827\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1187 - accuracy: 0.7876 - val_loss: 0.0145 - val_accuracy: 0.9794\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1217 - accuracy: 0.7761 - val_loss: 0.0155 - val_accuracy: 0.9783\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1157 - accuracy: 0.8012 - val_loss: 0.0150 - val_accuracy: 0.9819\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1129 - accuracy: 0.7973 - val_loss: 0.0157 - val_accuracy: 0.9773\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3945 - accuracy: 0.2147 - val_loss: 0.2837 - val_accuracy: 0.5995\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4482 - accuracy: 0.1942 - val_loss: 0.3010 - val_accuracy: 0.4884\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4709 - accuracy: 0.1662 - val_loss: 0.2939 - val_accuracy: 0.5440\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4584 - accuracy: 0.1828 - val_loss: 0.2947 - val_accuracy: 0.4632\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4030 - accuracy: 0.2574 - val_loss: 0.2657 - val_accuracy: 0.6115\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1064 - accuracy: 0.8058 - val_loss: 0.0134 - val_accuracy: 0.9809\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1050 - accuracy: 0.8070 - val_loss: 0.0160 - val_accuracy: 0.9779\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1010 - accuracy: 0.8139 - val_loss: 0.0122 - val_accuracy: 0.9840\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1049 - accuracy: 0.8019 - val_loss: 0.0151 - val_accuracy: 0.9793\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1050 - accuracy: 0.8038 - val_loss: 0.0136 - val_accuracy: 0.9789\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1218 - accuracy: 0.7681 - val_loss: 0.0121 - val_accuracy: 0.9831\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1156 - accuracy: 0.7826 - val_loss: 0.0125 - val_accuracy: 0.9825\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1156 - accuracy: 0.7840 - val_loss: 0.0140 - val_accuracy: 0.9806\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1258 - accuracy: 0.7596 - val_loss: 0.0138 - val_accuracy: 0.9815\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1262 - accuracy: 0.7601 - val_loss: 0.0163 - val_accuracy: 0.9751\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4330 - accuracy: 0.1405 - val_loss: 0.2941 - val_accuracy: 0.4263\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4563 - accuracy: 0.1225 - val_loss: 0.3123 - val_accuracy: 0.5277\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4645 - accuracy: 0.1369 - val_loss: 0.2957 - val_accuracy: 0.5245\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4730 - accuracy: 0.1358 - val_loss: 0.2959 - val_accuracy: 0.6671\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4628 - accuracy: 0.1366 - val_loss: 0.2996 - val_accuracy: 0.5389\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1122 - accuracy: 0.7918 - val_loss: 0.0137 - val_accuracy: 0.9799\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1188 - accuracy: 0.7761 - val_loss: 0.0139 - val_accuracy: 0.9803\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1216 - accuracy: 0.7713 - val_loss: 0.0146 - val_accuracy: 0.9775\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1154 - accuracy: 0.7873 - val_loss: 0.0137 - val_accuracy: 0.9790\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1138 - accuracy: 0.7886 - val_loss: 0.0155 - val_accuracy: 0.9755\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1357 - accuracy: 0.7397 - val_loss: 0.0128 - val_accuracy: 0.9824\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1304 - accuracy: 0.7484 - val_loss: 0.0148 - val_accuracy: 0.9766\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1317 - accuracy: 0.7451 - val_loss: 0.0131 - val_accuracy: 0.9811\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1509 - accuracy: 0.7124 - val_loss: 0.0150 - val_accuracy: 0.9789\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1406 - accuracy: 0.7321 - val_loss: 0.0150 - val_accuracy: 0.9777\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4376 - accuracy: 0.1142 - val_loss: 0.3122 - val_accuracy: 0.3988\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4322 - accuracy: 0.1291 - val_loss: 0.3047 - val_accuracy: 0.4670\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4402 - accuracy: 0.1101 - val_loss: 0.3130 - val_accuracy: 0.3974\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4298 - accuracy: 0.1233 - val_loss: 0.2986 - val_accuracy: 0.5565\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4521 - accuracy: 0.1195 - val_loss: 0.3120 - val_accuracy: 0.3701\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1311 - accuracy: 0.7608 - val_loss: 0.0192 - val_accuracy: 0.9721\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1413 - accuracy: 0.7420 - val_loss: 0.0158 - val_accuracy: 0.9797\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1346 - accuracy: 0.7521 - val_loss: 0.0161 - val_accuracy: 0.9779\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1261 - accuracy: 0.7681 - val_loss: 0.0150 - val_accuracy: 0.9807\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1278 - accuracy: 0.7704 - val_loss: 0.0165 - val_accuracy: 0.9758\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1749 - accuracy: 0.6711 - val_loss: 0.0178 - val_accuracy: 0.9777\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1645 - accuracy: 0.7093 - val_loss: 0.0195 - val_accuracy: 0.9747\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1662 - accuracy: 0.6857 - val_loss: 0.0191 - val_accuracy: 0.9733\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1655 - accuracy: 0.7038 - val_loss: 0.0188 - val_accuracy: 0.9757\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1737 - accuracy: 0.6764 - val_loss: 0.0211 - val_accuracy: 0.9697\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4934 - accuracy: 0.1291 - val_loss: 0.3142 - val_accuracy: 0.4020\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4620 - accuracy: 0.1230 - val_loss: 0.3115 - val_accuracy: 0.4117\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4881 - accuracy: 0.1856 - val_loss: 0.3150 - val_accuracy: 0.4910\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4900 - accuracy: 0.1385 - val_loss: 0.3170 - val_accuracy: 0.3659\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5004 - accuracy: 0.1217 - val_loss: 0.3179 - val_accuracy: 0.3334\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1288 - accuracy: 0.7593 - val_loss: 0.0171 - val_accuracy: 0.9753\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1317 - accuracy: 0.7580 - val_loss: 0.0185 - val_accuracy: 0.9737\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1411 - accuracy: 0.7407 - val_loss: 0.0158 - val_accuracy: 0.9785\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1371 - accuracy: 0.7445 - val_loss: 0.0154 - val_accuracy: 0.9789\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1367 - accuracy: 0.7486 - val_loss: 0.0183 - val_accuracy: 0.9722\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1527 - accuracy: 0.7115 - val_loss: 0.0154 - val_accuracy: 0.9784\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.1824 - accuracy: 0.6422 - val_loss: 0.0211 - val_accuracy: 0.9708\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1795 - accuracy: 0.6548 - val_loss: 0.0159 - val_accuracy: 0.9787\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1617 - accuracy: 0.7003 - val_loss: 0.0156 - val_accuracy: 0.9793\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1609 - accuracy: 0.7033 - val_loss: 0.0189 - val_accuracy: 0.9722\n",
            "188/188 [==============================] - 1s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4864 - accuracy: 0.1085 - val_loss: 0.3154 - val_accuracy: 0.4933\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4649 - accuracy: 0.1304 - val_loss: 0.3131 - val_accuracy: 0.3218\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5332 - accuracy: 0.1027 - val_loss: 0.3196 - val_accuracy: 0.3395\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5292 - accuracy: 0.1266 - val_loss: 0.3194 - val_accuracy: 0.4307\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5765 - accuracy: 0.1205 - val_loss: 0.3193 - val_accuracy: 0.3075\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1469 - accuracy: 0.7207 - val_loss: 0.0153 - val_accuracy: 0.9796\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1449 - accuracy: 0.7216 - val_loss: 0.0173 - val_accuracy: 0.9757\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1441 - accuracy: 0.7314 - val_loss: 0.0160 - val_accuracy: 0.9761\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1457 - accuracy: 0.7244 - val_loss: 0.0189 - val_accuracy: 0.9717\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1514 - accuracy: 0.7120 - val_loss: 0.0195 - val_accuracy: 0.9715\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.1745 - accuracy: 0.6596 - val_loss: 0.0163 - val_accuracy: 0.9784\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.1874 - accuracy: 0.6413 - val_loss: 0.0175 - val_accuracy: 0.9757\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1779 - accuracy: 0.6500 - val_loss: 0.0178 - val_accuracy: 0.9747\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1752 - accuracy: 0.6543 - val_loss: 0.0145 - val_accuracy: 0.9807\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1718 - accuracy: 0.6719 - val_loss: 0.0202 - val_accuracy: 0.9691\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5175 - accuracy: 0.1011 - val_loss: 0.3191 - val_accuracy: 0.1994\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5473 - accuracy: 0.1108 - val_loss: 0.3208 - val_accuracy: 0.1848\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4505 - accuracy: 0.1137 - val_loss: 0.3227 - val_accuracy: 0.2894\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5132 - accuracy: 0.1086 - val_loss: 0.3183 - val_accuracy: 0.2779\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.5518 - accuracy: 0.1064 - val_loss: 0.3202 - val_accuracy: 0.1248\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1789 - accuracy: 0.6831 - val_loss: 0.0292 - val_accuracy: 0.9596\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1801 - accuracy: 0.6775 - val_loss: 0.0257 - val_accuracy: 0.9629\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1736 - accuracy: 0.6955 - val_loss: 0.0274 - val_accuracy: 0.9617\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1738 - accuracy: 0.6850 - val_loss: 0.0221 - val_accuracy: 0.9707\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1794 - accuracy: 0.6722 - val_loss: 0.0282 - val_accuracy: 0.9592\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2317 - accuracy: 0.5680 - val_loss: 0.0283 - val_accuracy: 0.9623\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2109 - accuracy: 0.6260 - val_loss: 0.0264 - val_accuracy: 0.9664\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2366 - accuracy: 0.5810 - val_loss: 0.0270 - val_accuracy: 0.9658\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2263 - accuracy: 0.5955 - val_loss: 0.0290 - val_accuracy: 0.9624\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2267 - accuracy: 0.5671 - val_loss: 0.0301 - val_accuracy: 0.9602\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.6067 - accuracy: 0.0846 - val_loss: 0.3261 - val_accuracy: 0.0849\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5432 - accuracy: 0.0752 - val_loss: 0.3249 - val_accuracy: 0.1051\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.6232 - accuracy: 0.1288 - val_loss: 0.3258 - val_accuracy: 0.1581\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5383 - accuracy: 0.1017 - val_loss: 0.3200 - val_accuracy: 0.2123\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5848 - accuracy: 0.0987 - val_loss: 0.3317 - val_accuracy: 0.0991\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1682 - accuracy: 0.6895 - val_loss: 0.0197 - val_accuracy: 0.9725\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1715 - accuracy: 0.6877 - val_loss: 0.0201 - val_accuracy: 0.9735\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1715 - accuracy: 0.6786 - val_loss: 0.0210 - val_accuracy: 0.9711\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1664 - accuracy: 0.6894 - val_loss: 0.0201 - val_accuracy: 0.9742\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1825 - accuracy: 0.6599 - val_loss: 0.0289 - val_accuracy: 0.9555\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2581 - accuracy: 0.5071 - val_loss: 0.0222 - val_accuracy: 0.9714\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2330 - accuracy: 0.5544 - val_loss: 0.0236 - val_accuracy: 0.9695\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2355 - accuracy: 0.5438 - val_loss: 0.0259 - val_accuracy: 0.9649\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2173 - accuracy: 0.5731 - val_loss: 0.0213 - val_accuracy: 0.9724\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2411 - accuracy: 0.5390 - val_loss: 0.0273 - val_accuracy: 0.9617\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5774 - accuracy: 0.1031 - val_loss: 0.3259 - val_accuracy: 0.1372\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5899 - accuracy: 0.0903 - val_loss: 0.3286 - val_accuracy: 0.1589\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.6225 - accuracy: 0.1074 - val_loss: 0.3244 - val_accuracy: 0.1272\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5254 - accuracy: 0.1113 - val_loss: 0.3227 - val_accuracy: 0.1684\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5893 - accuracy: 0.1122 - val_loss: 0.3238 - val_accuracy: 0.1368\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1865 - accuracy: 0.6464 - val_loss: 0.0191 - val_accuracy: 0.9739\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1927 - accuracy: 0.6334 - val_loss: 0.0223 - val_accuracy: 0.9690\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1925 - accuracy: 0.6288 - val_loss: 0.0240 - val_accuracy: 0.9657\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1931 - accuracy: 0.6344 - val_loss: 0.0246 - val_accuracy: 0.9672\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1958 - accuracy: 0.6256 - val_loss: 0.0318 - val_accuracy: 0.9510\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2402 - accuracy: 0.5247 - val_loss: 0.0244 - val_accuracy: 0.9685\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2320 - accuracy: 0.5416 - val_loss: 0.0234 - val_accuracy: 0.9684\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2245 - accuracy: 0.5471 - val_loss: 0.0239 - val_accuracy: 0.9663\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2467 - accuracy: 0.5167 - val_loss: 0.0223 - val_accuracy: 0.9692\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2363 - accuracy: 0.5434 - val_loss: 0.0281 - val_accuracy: 0.9590\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5894 - accuracy: 0.1013 - val_loss: 0.3226 - val_accuracy: 0.1997\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.6136 - accuracy: 0.1052 - val_loss: 0.3278 - val_accuracy: 0.1291\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5190 - accuracy: 0.0930 - val_loss: 0.3247 - val_accuracy: 0.2600\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5296 - accuracy: 0.1036 - val_loss: 0.3226 - val_accuracy: 0.2157\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.6241 - accuracy: 0.0992 - val_loss: 0.3256 - val_accuracy: 0.0968\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0897 - accuracy: 0.8447 - val_loss: 0.0142 - val_accuracy: 0.9817\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0855 - accuracy: 0.8533 - val_loss: 0.0135 - val_accuracy: 0.9834\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0881 - accuracy: 0.8502 - val_loss: 0.0132 - val_accuracy: 0.9820\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0842 - accuracy: 0.8570 - val_loss: 0.0171 - val_accuracy: 0.9777\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0883 - accuracy: 0.8438 - val_loss: 0.0132 - val_accuracy: 0.9823\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1014 - accuracy: 0.8204 - val_loss: 0.0163 - val_accuracy: 0.9809\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1107 - accuracy: 0.8098 - val_loss: 0.0137 - val_accuracy: 0.9833\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1031 - accuracy: 0.8173 - val_loss: 0.0170 - val_accuracy: 0.9793\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1104 - accuracy: 0.7988 - val_loss: 0.0173 - val_accuracy: 0.9802\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1061 - accuracy: 0.8108 - val_loss: 0.0152 - val_accuracy: 0.9804\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3830 - accuracy: 0.2436 - val_loss: 0.2465 - val_accuracy: 0.6737\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4119 - accuracy: 0.2803 - val_loss: 0.2450 - val_accuracy: 0.6599\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3620 - accuracy: 0.3056 - val_loss: 0.2187 - val_accuracy: 0.7486\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3816 - accuracy: 0.3437 - val_loss: 0.2111 - val_accuracy: 0.7248\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3816 - accuracy: 0.2781 - val_loss: 0.2361 - val_accuracy: 0.7045\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0909 - accuracy: 0.8400 - val_loss: 0.0128 - val_accuracy: 0.9842\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0960 - accuracy: 0.8324 - val_loss: 0.0149 - val_accuracy: 0.9794\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0934 - accuracy: 0.8373 - val_loss: 0.0142 - val_accuracy: 0.9837\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0989 - accuracy: 0.8276 - val_loss: 0.0139 - val_accuracy: 0.9830\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0910 - accuracy: 0.8445 - val_loss: 0.0162 - val_accuracy: 0.9775\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1149 - accuracy: 0.7889 - val_loss: 0.0125 - val_accuracy: 0.9853\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1119 - accuracy: 0.8021 - val_loss: 0.0141 - val_accuracy: 0.9817\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1128 - accuracy: 0.7945 - val_loss: 0.0148 - val_accuracy: 0.9819\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1146 - accuracy: 0.7938 - val_loss: 0.0136 - val_accuracy: 0.9821\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1057 - accuracy: 0.8126 - val_loss: 0.0166 - val_accuracy: 0.9758\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3883 - accuracy: 0.1911 - val_loss: 0.2308 - val_accuracy: 0.6923\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3948 - accuracy: 0.1747 - val_loss: 0.2590 - val_accuracy: 0.6793\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.3967 - accuracy: 0.1782 - val_loss: 0.2581 - val_accuracy: 0.6633\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3879 - accuracy: 0.1699 - val_loss: 0.2768 - val_accuracy: 0.6169\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4008 - accuracy: 0.1618 - val_loss: 0.2745 - val_accuracy: 0.5842\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0983 - accuracy: 0.8281 - val_loss: 0.0140 - val_accuracy: 0.9817\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1095 - accuracy: 0.8051 - val_loss: 0.0142 - val_accuracy: 0.9810\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1071 - accuracy: 0.8084 - val_loss: 0.0149 - val_accuracy: 0.9812\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1045 - accuracy: 0.8141 - val_loss: 0.0124 - val_accuracy: 0.9840\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1032 - accuracy: 0.8197 - val_loss: 0.0180 - val_accuracy: 0.9741\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1202 - accuracy: 0.7825 - val_loss: 0.0143 - val_accuracy: 0.9803\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1170 - accuracy: 0.7865 - val_loss: 0.0132 - val_accuracy: 0.9835\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1221 - accuracy: 0.7761 - val_loss: 0.0141 - val_accuracy: 0.9823\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1281 - accuracy: 0.7605 - val_loss: 0.0178 - val_accuracy: 0.9749\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.1240 - accuracy: 0.7735 - val_loss: 0.0177 - val_accuracy: 0.9745\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4028 - accuracy: 0.1670 - val_loss: 0.2526 - val_accuracy: 0.7236\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.4015 - accuracy: 0.1275 - val_loss: 0.2897 - val_accuracy: 0.6181\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4268 - accuracy: 0.1345 - val_loss: 0.2815 - val_accuracy: 0.7035\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3862 - accuracy: 0.1506 - val_loss: 0.2681 - val_accuracy: 0.6786\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4072 - accuracy: 0.1472 - val_loss: 0.2633 - val_accuracy: 0.6556\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1151 - accuracy: 0.8035 - val_loss: 0.0168 - val_accuracy: 0.9794\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1162 - accuracy: 0.8014 - val_loss: 0.0148 - val_accuracy: 0.9830\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1167 - accuracy: 0.8003 - val_loss: 0.0149 - val_accuracy: 0.9818\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1142 - accuracy: 0.8096 - val_loss: 0.0160 - val_accuracy: 0.9798\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1184 - accuracy: 0.7949 - val_loss: 0.0281 - val_accuracy: 0.9638\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1384 - accuracy: 0.7600 - val_loss: 0.0152 - val_accuracy: 0.9823\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.1349 - accuracy: 0.7561 - val_loss: 0.0179 - val_accuracy: 0.9777\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1465 - accuracy: 0.7450 - val_loss: 0.0202 - val_accuracy: 0.9768\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1492 - accuracy: 0.7258 - val_loss: 0.0186 - val_accuracy: 0.9784\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1448 - accuracy: 0.7424 - val_loss: 0.0176 - val_accuracy: 0.9772\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 3ms/step - loss: 0.4278 - accuracy: 0.1564 - val_loss: 0.2992 - val_accuracy: 0.5163\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4797 - accuracy: 0.1465 - val_loss: 0.3030 - val_accuracy: 0.5377\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4630 - accuracy: 0.1443 - val_loss: 0.3023 - val_accuracy: 0.4972\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4530 - accuracy: 0.2196 - val_loss: 0.3033 - val_accuracy: 0.5590\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4342 - accuracy: 0.1185 - val_loss: 0.3143 - val_accuracy: 0.4665\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1227 - accuracy: 0.7820 - val_loss: 0.0145 - val_accuracy: 0.9818\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1277 - accuracy: 0.7771 - val_loss: 0.0169 - val_accuracy: 0.9786\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1236 - accuracy: 0.7872 - val_loss: 0.0154 - val_accuracy: 0.9802\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1254 - accuracy: 0.7810 - val_loss: 0.0145 - val_accuracy: 0.9830\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1227 - accuracy: 0.7849 - val_loss: 0.0196 - val_accuracy: 0.9736\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1406 - accuracy: 0.7458 - val_loss: 0.0173 - val_accuracy: 0.9794\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1430 - accuracy: 0.7348 - val_loss: 0.0182 - val_accuracy: 0.9769\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1547 - accuracy: 0.7180 - val_loss: 0.0167 - val_accuracy: 0.9781\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1482 - accuracy: 0.7208 - val_loss: 0.0174 - val_accuracy: 0.9778\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1611 - accuracy: 0.7018 - val_loss: 0.0182 - val_accuracy: 0.9748\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4560 - accuracy: 0.1652 - val_loss: 0.2965 - val_accuracy: 0.6125\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4269 - accuracy: 0.1428 - val_loss: 0.3055 - val_accuracy: 0.5327\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4386 - accuracy: 0.1305 - val_loss: 0.3093 - val_accuracy: 0.5561\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4727 - accuracy: 0.1294 - val_loss: 0.3066 - val_accuracy: 0.5114\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4645 - accuracy: 0.1239 - val_loss: 0.3090 - val_accuracy: 0.5165\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1354 - accuracy: 0.7535 - val_loss: 0.0160 - val_accuracy: 0.9805\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1390 - accuracy: 0.7443 - val_loss: 0.0217 - val_accuracy: 0.9728\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1315 - accuracy: 0.7599 - val_loss: 0.0158 - val_accuracy: 0.9796\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1292 - accuracy: 0.7692 - val_loss: 0.0154 - val_accuracy: 0.9805\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1296 - accuracy: 0.7716 - val_loss: 0.0191 - val_accuracy: 0.9733\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1599 - accuracy: 0.6963 - val_loss: 0.0193 - val_accuracy: 0.9768\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1600 - accuracy: 0.6909 - val_loss: 0.0163 - val_accuracy: 0.9807\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1515 - accuracy: 0.7179 - val_loss: 0.0155 - val_accuracy: 0.9792\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1699 - accuracy: 0.6751 - val_loss: 0.0172 - val_accuracy: 0.9771\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1569 - accuracy: 0.7075 - val_loss: 0.0175 - val_accuracy: 0.9759\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4671 - accuracy: 0.0987 - val_loss: 0.3182 - val_accuracy: 0.3754\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.5005 - accuracy: 0.1015 - val_loss: 0.3147 - val_accuracy: 0.3606\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4723 - accuracy: 0.1118 - val_loss: 0.3091 - val_accuracy: 0.4370\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.5010 - accuracy: 0.1119 - val_loss: 0.3151 - val_accuracy: 0.4557\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.4477 - accuracy: 0.1350 - val_loss: 0.3111 - val_accuracy: 0.5217\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1594 - accuracy: 0.7234 - val_loss: 0.0212 - val_accuracy: 0.9750\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1597 - accuracy: 0.7246 - val_loss: 0.0213 - val_accuracy: 0.9746\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1574 - accuracy: 0.7303 - val_loss: 0.0213 - val_accuracy: 0.9751\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1627 - accuracy: 0.7206 - val_loss: 0.0187 - val_accuracy: 0.9768\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1642 - accuracy: 0.7135 - val_loss: 0.0227 - val_accuracy: 0.9682\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2093 - accuracy: 0.6062 - val_loss: 0.0235 - val_accuracy: 0.9702\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2034 - accuracy: 0.6296 - val_loss: 0.0297 - val_accuracy: 0.9608\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.1927 - accuracy: 0.6494 - val_loss: 0.0244 - val_accuracy: 0.9716\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1939 - accuracy: 0.6595 - val_loss: 0.0227 - val_accuracy: 0.9739\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2037 - accuracy: 0.6236 - val_loss: 0.0242 - val_accuracy: 0.9703\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.4852 - accuracy: 0.1396 - val_loss: 0.3182 - val_accuracy: 0.3716\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5116 - accuracy: 0.0832 - val_loss: 0.3185 - val_accuracy: 0.2846\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.4864 - accuracy: 0.0980 - val_loss: 0.3217 - val_accuracy: 0.2092\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.4861 - accuracy: 0.1332 - val_loss: 0.3178 - val_accuracy: 0.2989\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5055 - accuracy: 0.2057 - val_loss: 0.3117 - val_accuracy: 0.4390\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1690 - accuracy: 0.7016 - val_loss: 0.0203 - val_accuracy: 0.9743\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1694 - accuracy: 0.7064 - val_loss: 0.0202 - val_accuracy: 0.9753\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1624 - accuracy: 0.7054 - val_loss: 0.0196 - val_accuracy: 0.9768\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1590 - accuracy: 0.7203 - val_loss: 0.0215 - val_accuracy: 0.9699\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1708 - accuracy: 0.6879 - val_loss: 0.0243 - val_accuracy: 0.9675\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2021 - accuracy: 0.6298 - val_loss: 0.0211 - val_accuracy: 0.9747\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2109 - accuracy: 0.6146 - val_loss: 0.0225 - val_accuracy: 0.9717\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2014 - accuracy: 0.6280 - val_loss: 0.0214 - val_accuracy: 0.9745\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.2122 - accuracy: 0.6181 - val_loss: 0.0243 - val_accuracy: 0.9697\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2029 - accuracy: 0.6165 - val_loss: 0.0258 - val_accuracy: 0.9681\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.4792 - accuracy: 0.1462 - val_loss: 0.3149 - val_accuracy: 0.4128\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5163 - accuracy: 0.1164 - val_loss: 0.3186 - val_accuracy: 0.2733\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.4971 - accuracy: 0.1014 - val_loss: 0.3225 - val_accuracy: 0.2184\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5149 - accuracy: 0.1182 - val_loss: 0.3139 - val_accuracy: 0.3880\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5350 - accuracy: 0.0903 - val_loss: 0.3228 - val_accuracy: 0.2864\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1751 - accuracy: 0.6818 - val_loss: 0.0225 - val_accuracy: 0.9710\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1810 - accuracy: 0.6684 - val_loss: 0.0211 - val_accuracy: 0.9745\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1717 - accuracy: 0.6900 - val_loss: 0.0200 - val_accuracy: 0.9754\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1787 - accuracy: 0.6733 - val_loss: 0.0213 - val_accuracy: 0.9726\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1678 - accuracy: 0.6995 - val_loss: 0.0258 - val_accuracy: 0.9628\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2106 - accuracy: 0.5872 - val_loss: 0.0221 - val_accuracy: 0.9720\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2058 - accuracy: 0.6027 - val_loss: 0.0232 - val_accuracy: 0.9705\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2182 - accuracy: 0.5713 - val_loss: 0.0222 - val_accuracy: 0.9727\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2198 - accuracy: 0.5634 - val_loss: 0.0220 - val_accuracy: 0.9726\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2268 - accuracy: 0.5474 - val_loss: 0.0285 - val_accuracy: 0.9621\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5212 - accuracy: 0.1065 - val_loss: 0.3218 - val_accuracy: 0.2549\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5418 - accuracy: 0.0993 - val_loss: 0.3200 - val_accuracy: 0.3183\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.4881 - accuracy: 0.1051 - val_loss: 0.3230 - val_accuracy: 0.1791\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5244 - accuracy: 0.1037 - val_loss: 0.3243 - val_accuracy: 0.1314\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5392 - accuracy: 0.1078 - val_loss: 0.3169 - val_accuracy: 0.3080\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1086 - accuracy: 0.8168 - val_loss: 0.0121 - val_accuracy: 0.9830\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1067 - accuracy: 0.8285 - val_loss: 0.0129 - val_accuracy: 0.9817\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0965 - accuracy: 0.8461 - val_loss: 0.0126 - val_accuracy: 0.9825\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0974 - accuracy: 0.8441 - val_loss: 0.0157 - val_accuracy: 0.9764\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.0961 - accuracy: 0.8416 - val_loss: 0.0162 - val_accuracy: 0.9751\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1457 - accuracy: 0.7693 - val_loss: 0.0173 - val_accuracy: 0.9777\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1372 - accuracy: 0.7734 - val_loss: 0.0176 - val_accuracy: 0.9767\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1325 - accuracy: 0.7802 - val_loss: 0.0153 - val_accuracy: 0.9806\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1415 - accuracy: 0.7578 - val_loss: 0.0150 - val_accuracy: 0.9827\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1376 - accuracy: 0.7719 - val_loss: 0.0186 - val_accuracy: 0.9756\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4191 - accuracy: 0.2096 - val_loss: 0.2966 - val_accuracy: 0.6106\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3825 - accuracy: 0.2577 - val_loss: 0.2526 - val_accuracy: 0.6313\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3984 - accuracy: 0.2118 - val_loss: 0.2869 - val_accuracy: 0.5628\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3695 - accuracy: 0.2988 - val_loss: 0.2418 - val_accuracy: 0.7154\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4100 - accuracy: 0.2033 - val_loss: 0.2924 - val_accuracy: 0.5059\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 6s 4ms/step - loss: 0.1051 - accuracy: 0.8293 - val_loss: 0.0114 - val_accuracy: 0.9833\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1098 - accuracy: 0.8176 - val_loss: 0.0140 - val_accuracy: 0.9801\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1016 - accuracy: 0.8364 - val_loss: 0.0138 - val_accuracy: 0.9800\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1038 - accuracy: 0.8306 - val_loss: 0.0138 - val_accuracy: 0.9795\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1006 - accuracy: 0.8364 - val_loss: 0.0136 - val_accuracy: 0.9802\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1384 - accuracy: 0.7506 - val_loss: 0.0143 - val_accuracy: 0.9822\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1370 - accuracy: 0.7532 - val_loss: 0.0145 - val_accuracy: 0.9825\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1333 - accuracy: 0.7702 - val_loss: 0.0145 - val_accuracy: 0.9810\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1448 - accuracy: 0.7418 - val_loss: 0.0148 - val_accuracy: 0.9809\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1426 - accuracy: 0.7471 - val_loss: 0.0169 - val_accuracy: 0.9754\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4202 - accuracy: 0.1442 - val_loss: 0.2957 - val_accuracy: 0.5794\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3998 - accuracy: 0.1363 - val_loss: 0.2998 - val_accuracy: 0.5119\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4107 - accuracy: 0.1356 - val_loss: 0.2977 - val_accuracy: 0.5417\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3912 - accuracy: 0.1547 - val_loss: 0.2879 - val_accuracy: 0.6438\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4250 - accuracy: 0.1293 - val_loss: 0.3023 - val_accuracy: 0.6021\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1197 - accuracy: 0.7973 - val_loss: 0.0131 - val_accuracy: 0.9807\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1157 - accuracy: 0.8030 - val_loss: 0.0131 - val_accuracy: 0.9801\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1158 - accuracy: 0.8034 - val_loss: 0.0125 - val_accuracy: 0.9824\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1152 - accuracy: 0.7994 - val_loss: 0.0131 - val_accuracy: 0.9806\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1199 - accuracy: 0.8014 - val_loss: 0.0161 - val_accuracy: 0.9745\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1538 - accuracy: 0.7160 - val_loss: 0.0136 - val_accuracy: 0.9832\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1446 - accuracy: 0.7322 - val_loss: 0.0152 - val_accuracy: 0.9799\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1452 - accuracy: 0.7352 - val_loss: 0.0139 - val_accuracy: 0.9827\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.1466 - accuracy: 0.7330 - val_loss: 0.0169 - val_accuracy: 0.9764\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 3ms/step - loss: 0.1508 - accuracy: 0.7261 - val_loss: 0.0186 - val_accuracy: 0.9721\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.3960 - accuracy: 0.1198 - val_loss: 0.3073 - val_accuracy: 0.5611\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.4034 - accuracy: 0.1237 - val_loss: 0.2966 - val_accuracy: 0.6216\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4348 - accuracy: 0.1214 - val_loss: 0.3101 - val_accuracy: 0.4480\n",
            "375/375 [==============================] - 1s 1ms/step\n",
            "1200/1200 [==============================] - 5s 4ms/step - loss: 0.4248 - accuracy: 0.1298 - val_loss: 0.2980 - val_accuracy: 0.5109\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "1200/1200 [==============================] - 4s 3ms/step - loss: 0.4079 - accuracy: 0.1343 - val_loss: 0.2984 - val_accuracy: 0.5221\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1339 - accuracy: 0.7803 - val_loss: 0.0154 - val_accuracy: 0.9814\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1374 - accuracy: 0.7689 - val_loss: 0.0158 - val_accuracy: 0.9789\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1310 - accuracy: 0.7817 - val_loss: 0.0235 - val_accuracy: 0.9646\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1355 - accuracy: 0.7763 - val_loss: 0.0166 - val_accuracy: 0.9774\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1359 - accuracy: 0.7787 - val_loss: 0.0201 - val_accuracy: 0.9714\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1890 - accuracy: 0.6795 - val_loss: 0.0227 - val_accuracy: 0.9753\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1932 - accuracy: 0.6850 - val_loss: 0.0250 - val_accuracy: 0.9749\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1709 - accuracy: 0.7106 - val_loss: 0.0210 - val_accuracy: 0.9781\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1770 - accuracy: 0.6987 - val_loss: 0.0262 - val_accuracy: 0.9723\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1909 - accuracy: 0.6728 - val_loss: 0.0256 - val_accuracy: 0.9701\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4550 - accuracy: 0.1579 - val_loss: 0.3132 - val_accuracy: 0.3413\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4385 - accuracy: 0.1220 - val_loss: 0.3092 - val_accuracy: 0.4094\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.4805 - accuracy: 0.1349 - val_loss: 0.3150 - val_accuracy: 0.4208\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.4480 - accuracy: 0.1456 - val_loss: 0.3098 - val_accuracy: 0.4584\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4760 - accuracy: 0.1124 - val_loss: 0.3159 - val_accuracy: 0.3450\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1379 - accuracy: 0.7734 - val_loss: 0.0189 - val_accuracy: 0.9734\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1389 - accuracy: 0.7641 - val_loss: 0.0170 - val_accuracy: 0.9772\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1351 - accuracy: 0.7776 - val_loss: 0.0149 - val_accuracy: 0.9810\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1400 - accuracy: 0.7621 - val_loss: 0.0151 - val_accuracy: 0.9806\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1368 - accuracy: 0.7698 - val_loss: 0.0175 - val_accuracy: 0.9752\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1878 - accuracy: 0.6519 - val_loss: 0.0199 - val_accuracy: 0.9780\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1828 - accuracy: 0.6631 - val_loss: 0.0195 - val_accuracy: 0.9779\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1893 - accuracy: 0.6479 - val_loss: 0.0199 - val_accuracy: 0.9776\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1813 - accuracy: 0.6717 - val_loss: 0.0205 - val_accuracy: 0.9765\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1702 - accuracy: 0.6898 - val_loss: 0.0207 - val_accuracy: 0.9746\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4463 - accuracy: 0.1002 - val_loss: 0.3136 - val_accuracy: 0.3916\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.5118 - accuracy: 0.1284 - val_loss: 0.3120 - val_accuracy: 0.3827\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.4854 - accuracy: 0.1068 - val_loss: 0.3179 - val_accuracy: 0.2764\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4772 - accuracy: 0.1075 - val_loss: 0.3183 - val_accuracy: 0.3117\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.4877 - accuracy: 0.1288 - val_loss: 0.3146 - val_accuracy: 0.3330\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1442 - accuracy: 0.7542 - val_loss: 0.0173 - val_accuracy: 0.9760\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1455 - accuracy: 0.7482 - val_loss: 0.0178 - val_accuracy: 0.9759\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1428 - accuracy: 0.7627 - val_loss: 0.0165 - val_accuracy: 0.9780\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1453 - accuracy: 0.7470 - val_loss: 0.0155 - val_accuracy: 0.9794\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1419 - accuracy: 0.7582 - val_loss: 0.0211 - val_accuracy: 0.9684\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1909 - accuracy: 0.6343 - val_loss: 0.0206 - val_accuracy: 0.9752\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1876 - accuracy: 0.6485 - val_loss: 0.0193 - val_accuracy: 0.9785\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1966 - accuracy: 0.6266 - val_loss: 0.0198 - val_accuracy: 0.9768\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1900 - accuracy: 0.6419 - val_loss: 0.0198 - val_accuracy: 0.9759\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.1817 - accuracy: 0.6619 - val_loss: 0.0244 - val_accuracy: 0.9676\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4626 - accuracy: 0.1205 - val_loss: 0.3123 - val_accuracy: 0.4905\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4745 - accuracy: 0.1074 - val_loss: 0.3151 - val_accuracy: 0.2727\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.4663 - accuracy: 0.1054 - val_loss: 0.3164 - val_accuracy: 0.2960\n",
            "188/188 [==============================] - 0s 1ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4671 - accuracy: 0.1204 - val_loss: 0.3170 - val_accuracy: 0.4053\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "600/600 [==============================] - 2s 4ms/step - loss: 0.4850 - accuracy: 0.1095 - val_loss: 0.3176 - val_accuracy: 0.2359\n",
            "188/188 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1755 - accuracy: 0.7062 - val_loss: 0.0265 - val_accuracy: 0.9688\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1829 - accuracy: 0.6841 - val_loss: 0.0284 - val_accuracy: 0.9674\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1710 - accuracy: 0.7149 - val_loss: 0.0243 - val_accuracy: 0.9725\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1757 - accuracy: 0.7087 - val_loss: 0.0243 - val_accuracy: 0.9735\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1793 - accuracy: 0.6938 - val_loss: 0.0274 - val_accuracy: 0.9667\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2476 - accuracy: 0.5693 - val_loss: 0.0503 - val_accuracy: 0.9640\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2297 - accuracy: 0.6024 - val_loss: 0.0446 - val_accuracy: 0.9641\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2475 - accuracy: 0.5683 - val_loss: 0.0473 - val_accuracy: 0.9632\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2342 - accuracy: 0.5975 - val_loss: 0.0440 - val_accuracy: 0.9627\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2300 - accuracy: 0.5677 - val_loss: 0.0395 - val_accuracy: 0.9600\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5088 - accuracy: 0.1176 - val_loss: 0.3192 - val_accuracy: 0.3056\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.5182 - accuracy: 0.1115 - val_loss: 0.3213 - val_accuracy: 0.1975\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.5489 - accuracy: 0.0952 - val_loss: 0.3228 - val_accuracy: 0.1299\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.4980 - accuracy: 0.1666 - val_loss: 0.3175 - val_accuracy: 0.3332\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.4878 - accuracy: 0.1228 - val_loss: 0.3180 - val_accuracy: 0.2330\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1798 - accuracy: 0.6927 - val_loss: 0.0231 - val_accuracy: 0.9702\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.1802 - accuracy: 0.6945 - val_loss: 0.0231 - val_accuracy: 0.9748\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1783 - accuracy: 0.6935 - val_loss: 0.0256 - val_accuracy: 0.9676\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1758 - accuracy: 0.7050 - val_loss: 0.0231 - val_accuracy: 0.9721\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1802 - accuracy: 0.6826 - val_loss: 0.0273 - val_accuracy: 0.9630\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2482 - accuracy: 0.5302 - val_loss: 0.0401 - val_accuracy: 0.9575\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2332 - accuracy: 0.5423 - val_loss: 0.0326 - val_accuracy: 0.9680\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2524 - accuracy: 0.5039 - val_loss: 0.0362 - val_accuracy: 0.9622\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2447 - accuracy: 0.5349 - val_loss: 0.0338 - val_accuracy: 0.9714\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2390 - accuracy: 0.5372 - val_loss: 0.0368 - val_accuracy: 0.9572\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.5609 - accuracy: 0.1019 - val_loss: 0.3225 - val_accuracy: 0.2026\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.5261 - accuracy: 0.1087 - val_loss: 0.3235 - val_accuracy: 0.2136\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.5436 - accuracy: 0.1182 - val_loss: 0.3198 - val_accuracy: 0.2760\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5415 - accuracy: 0.0775 - val_loss: 0.3212 - val_accuracy: 0.1736\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.4928 - accuracy: 0.1035 - val_loss: 0.3183 - val_accuracy: 0.2149\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1893 - accuracy: 0.6630 - val_loss: 0.0228 - val_accuracy: 0.9716\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.1821 - accuracy: 0.6905 - val_loss: 0.0228 - val_accuracy: 0.9729\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1828 - accuracy: 0.6833 - val_loss: 0.0223 - val_accuracy: 0.9753\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1843 - accuracy: 0.6785 - val_loss: 0.0229 - val_accuracy: 0.9705\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.1842 - accuracy: 0.6740 - val_loss: 0.0262 - val_accuracy: 0.9667\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2466 - accuracy: 0.5070 - val_loss: 0.0321 - val_accuracy: 0.9676\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2525 - accuracy: 0.4981 - val_loss: 0.0326 - val_accuracy: 0.9669\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2581 - accuracy: 0.4860 - val_loss: 0.0337 - val_accuracy: 0.9642\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2479 - accuracy: 0.5027 - val_loss: 0.0352 - val_accuracy: 0.9656\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.2643 - accuracy: 0.4649 - val_loss: 0.0358 - val_accuracy: 0.9625\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.5835 - accuracy: 0.0881 - val_loss: 0.3249 - val_accuracy: 0.2015\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.5378 - accuracy: 0.0909 - val_loss: 0.3219 - val_accuracy: 0.1882\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 4ms/step - loss: 0.5538 - accuracy: 0.1148 - val_loss: 0.3217 - val_accuracy: 0.1719\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.5002 - accuracy: 0.1084 - val_loss: 0.3173 - val_accuracy: 0.2947\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.5439 - accuracy: 0.0983 - val_loss: 0.3265 - val_accuracy: 0.1644\n",
            "94/94 [==============================] - 0s 2ms/step\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0746 - accuracy: 0.8757 - val_loss: 0.0142 - val_accuracy: 0.9818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7fd82f39ef90>,\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'activation': ['relu', 'linear', 'tanh'],\n",
              "                         'batch_size': [32, 64, 128],\n",
              "                         'dropout_rate': [0, 0.25, 0.5],\n",
              "                         'optimizer': ['rmsprop', 'adam', 'sgd']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=make_scorer(scoring), verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oni25KKn6IEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d74c82-4c0b-4849-b30f-d6c2e3033b04"
      },
      "source": [
        "clf.best_score_, clf.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9813166666666667,\n",
              " {'activation': 'linear',\n",
              "  'batch_size': 32,\n",
              "  'dropout_rate': 0,\n",
              "  'optimizer': 'rmsprop'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x35DxIbU6VI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a694f423-fd96-4906-ae16-caa249e46249"
      },
      "source": [
        "clf.score(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109/313 [=========>....................] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9847"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}